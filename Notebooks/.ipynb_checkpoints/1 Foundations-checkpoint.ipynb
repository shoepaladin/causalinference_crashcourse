{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference Examples\n",
    "# 1 Foundations\n",
    "Julian Hsu\n",
    "Date Made: 5 Aug 2021 \n",
    "\n",
    "### Table of Contents with Navigation Links\n",
    "* [Write Causal Models](#Section1)\n",
    "* [Simulate Data](#Section2)\n",
    "* [Bootstrapping Examples](#Section3)\n",
    "* [Bootstrapping Examples - unconfoundedness violation](#Section4)\n",
    "* [Bootstrapping Examples - overlap violation](#Section5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os \n",
    "\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.discrete.conditional_models import ConditionalLogit\n",
    "\n",
    "from IPython.display import display    \n",
    "\n",
    "\n",
    "import scipy.stats \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, LassoCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section1'></a>\n",
    "\n",
    "## Write Causal Models\n",
    "Write several functions here for estimate HTE. Each model _must_ do datasplitting.\n",
    "These functions will do a lot of predictions, so try to standardize the prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stnomics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section2'></a>\n",
    "\n",
    "## Bring in Simulated Data\n",
    "Pretend we've never seen this data before, and do balance checks between treatment and control \n",
    "\n",
    "For fun, use the Friedman function: https://www.sfu.ca/~ssurjano/fried.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    N = 2000\n",
    "    \n",
    "    cov = [[1.00, 0.08, 0.05, 0.05],\n",
    "           [0.08, 1.00,-0.08,-0.02],\n",
    "           [0.05,-0.08, 1.00,-0.10],\n",
    "           [0.05,-0.02,-0.10, 1.00]]\n",
    "    cov = np.eye(4)\n",
    "    X = np.random.multivariate_normal(np.zeros(4), cov,N)\n",
    "    x1,x2,x3,x4= X[:,0],X[:,1],X[:,2],X[:,3]\n",
    "\n",
    "    treatment_latent = 2*np.sin( np.pi * x4 * x3) + 10*(x2-0.5)**2 - 10*x1\n",
    "    m,s = np.average(treatment_latent), np.std(treatment_latent)\n",
    "\n",
    "    treatment_latent = (treatment_latent - m) / s\n",
    "    \n",
    "    random_t = np.random.normal(0,1,N)\n",
    "    \n",
    "    treatment_latent += random_t\n",
    "    \n",
    "    treatment = np.array( np.exp(treatment_latent) / (1+ np.exp(treatment_latent)) > np.random.uniform(0,1,N) ).astype(np.int32)\n",
    "\n",
    "#     Y = 100 +0.5*x1 - 6*x2 + -2*x4*x1 + 0.5*x1*x2 - 7*(x3+1)**(0.5) + 8/(0.5+x3+x4)\n",
    "    Y = 100 + 10*np.sin( np.pi * x1 * x2) + 20*(x3-0.5)**2 - 10*x4\n",
    "#     GT = np.std(Y)\n",
    "    random_y = np.random.normal(0,1,N)\n",
    "\n",
    "    GT = 5\n",
    "    Y += np.random.normal(1,2,N)\n",
    "    Y += GT*(treatment==1) \n",
    "    \n",
    "    df_est = pd.DataFrame({'x1':x1, 'x2':x2,'x3':x3,'x4':x4,'treatment':treatment, 'Y':Y, 'GT':GT} )\n",
    "    df_est['x1_2'] = df_est['x1'].pow(2)\n",
    "    df_est['x2_2'] = df_est['x2'].pow(2)\n",
    "    df_est['x3_2'] = df_est['x3'].pow(2)\n",
    "    df_est['x4_2'] = df_est['x4'].pow(2)    \n",
    "    return df_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>treatment</th>\n",
       "      <th>Y</th>\n",
       "      <th>GT</th>\n",
       "      <th>x1_2</th>\n",
       "      <th>x2_2</th>\n",
       "      <th>x3_2</th>\n",
       "      <th>x4_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.313283</td>\n",
       "      <td>-0.489695</td>\n",
       "      <td>0.563628</td>\n",
       "      <td>-0.324982</td>\n",
       "      <td>1</td>\n",
       "      <td>105.700209</td>\n",
       "      <td>5</td>\n",
       "      <td>0.098146</td>\n",
       "      <td>0.239801</td>\n",
       "      <td>0.317676</td>\n",
       "      <td>0.105613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.347911</td>\n",
       "      <td>0.829529</td>\n",
       "      <td>0.863875</td>\n",
       "      <td>-0.082179</td>\n",
       "      <td>1</td>\n",
       "      <td>102.593495</td>\n",
       "      <td>5</td>\n",
       "      <td>0.121042</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>0.746279</td>\n",
       "      <td>0.006753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.674069</td>\n",
       "      <td>1.074357</td>\n",
       "      <td>0.852527</td>\n",
       "      <td>0.582977</td>\n",
       "      <td>0</td>\n",
       "      <td>105.036677</td>\n",
       "      <td>5</td>\n",
       "      <td>2.802507</td>\n",
       "      <td>1.154242</td>\n",
       "      <td>0.726802</td>\n",
       "      <td>0.339862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.288611</td>\n",
       "      <td>-0.335995</td>\n",
       "      <td>-1.054658</td>\n",
       "      <td>-0.091539</td>\n",
       "      <td>0</td>\n",
       "      <td>144.336512</td>\n",
       "      <td>5</td>\n",
       "      <td>0.083296</td>\n",
       "      <td>0.112893</td>\n",
       "      <td>1.112303</td>\n",
       "      <td>0.008379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.972706</td>\n",
       "      <td>0.119425</td>\n",
       "      <td>-1.156609</td>\n",
       "      <td>-0.030407</td>\n",
       "      <td>0</td>\n",
       "      <td>152.401670</td>\n",
       "      <td>5</td>\n",
       "      <td>0.946158</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>1.337744</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.644172</td>\n",
       "      <td>-0.381519</td>\n",
       "      <td>2.104919</td>\n",
       "      <td>-0.124986</td>\n",
       "      <td>0</td>\n",
       "      <td>158.054612</td>\n",
       "      <td>5</td>\n",
       "      <td>0.414957</td>\n",
       "      <td>0.145557</td>\n",
       "      <td>4.430683</td>\n",
       "      <td>0.015621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.018871</td>\n",
       "      <td>-1.890406</td>\n",
       "      <td>0.717081</td>\n",
       "      <td>1.377236</td>\n",
       "      <td>1</td>\n",
       "      <td>96.820982</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>3.573634</td>\n",
       "      <td>0.514205</td>\n",
       "      <td>1.896778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.782518</td>\n",
       "      <td>-0.175322</td>\n",
       "      <td>-0.079507</td>\n",
       "      <td>0.643935</td>\n",
       "      <td>1</td>\n",
       "      <td>97.958290</td>\n",
       "      <td>5</td>\n",
       "      <td>0.612335</td>\n",
       "      <td>0.030738</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.414652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-1.698061</td>\n",
       "      <td>-0.853848</td>\n",
       "      <td>0.383134</td>\n",
       "      <td>1.211971</td>\n",
       "      <td>1</td>\n",
       "      <td>81.896919</td>\n",
       "      <td>5</td>\n",
       "      <td>2.883411</td>\n",
       "      <td>0.729056</td>\n",
       "      <td>0.146792</td>\n",
       "      <td>1.468875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-1.783112</td>\n",
       "      <td>0.654464</td>\n",
       "      <td>0.445458</td>\n",
       "      <td>-0.876325</td>\n",
       "      <td>1</td>\n",
       "      <td>119.786695</td>\n",
       "      <td>5</td>\n",
       "      <td>3.179490</td>\n",
       "      <td>0.428324</td>\n",
       "      <td>0.198432</td>\n",
       "      <td>0.767946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4  treatment           Y  GT  \\\n",
       "0     0.313283 -0.489695  0.563628 -0.324982          1  105.700209   5   \n",
       "1    -0.347911  0.829529  0.863875 -0.082179          1  102.593495   5   \n",
       "2    -1.674069  1.074357  0.852527  0.582977          0  105.036677   5   \n",
       "3     0.288611 -0.335995 -1.054658 -0.091539          0  144.336512   5   \n",
       "4    -0.972706  0.119425 -1.156609 -0.030407          0  152.401670   5   \n",
       "...        ...       ...       ...       ...        ...         ...  ..   \n",
       "1995 -0.644172 -0.381519  2.104919 -0.124986          0  158.054612   5   \n",
       "1996 -0.018871 -1.890406  0.717081  1.377236          1   96.820982   5   \n",
       "1997  0.782518 -0.175322 -0.079507  0.643935          1   97.958290   5   \n",
       "1998 -1.698061 -0.853848  0.383134  1.211971          1   81.896919   5   \n",
       "1999 -1.783112  0.654464  0.445458 -0.876325          1  119.786695   5   \n",
       "\n",
       "          x1_2      x2_2      x3_2      x4_2  \n",
       "0     0.098146  0.239801  0.317676  0.105613  \n",
       "1     0.121042  0.688119  0.746279  0.006753  \n",
       "2     2.802507  1.154242  0.726802  0.339862  \n",
       "3     0.083296  0.112893  1.112303  0.008379  \n",
       "4     0.946158  0.014262  1.337744  0.000925  \n",
       "...        ...       ...       ...       ...  \n",
       "1995  0.414957  0.145557  4.430683  0.015621  \n",
       "1996  0.000356  3.573634  0.514205  1.896778  \n",
       "1997  0.612335  0.030738  0.006321  0.414652  \n",
       "1998  2.883411  0.729056  0.146792  1.468875  \n",
       "1999  3.179490  0.428324  0.198432  0.767946  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_iter = 500\n",
    "## treatment prediction models\n",
    "t_models = {}\n",
    "t_models['LogitCV'] = LogisticRegressionCV(cv=5, random_state=27, n_jobs=-1)\n",
    "t_models['logit'] = LogisticRegression(penalty='l2',solver='lbfgs', C=1, max_iter=model_max_iter, fit_intercept=True)\n",
    "t_models['logit_L1_C2'] = LogisticRegression(penalty='l1',C=2, max_iter=model_max_iter, fit_intercept=True)\n",
    "t_models['logit_L2_C5'] = LogisticRegression(penalty='l2',C=2, max_iter=model_max_iter, fit_intercept=True)\n",
    "t_models['rf_md10'] = RandomForestClassifier(n_estimators=25,max_depth=10, min_samples_split=200,n_jobs=-1)\n",
    "t_models['rf_md3'] = RandomForestClassifier(n_estimators=25,max_depth=3, min_samples_split=200,n_jobs=-1)\n",
    "t_models['nn'] = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1,max_iter=model_max_iter)\n",
    "## outcome prediction models\n",
    "y_models = {}\n",
    "y_models['LassoCV'] = LassoCV(cv=5, n_jobs=-1,  random_state=27)\n",
    "y_models['ols'] = LinearRegression()\n",
    "y_models['lasso_a2'] = Lasso(alpha=2,max_iter=model_max_iter)\n",
    "y_models['ridge_a2'] = Ridge(alpha=2,max_iter=model_max_iter)\n",
    "y_models['rf_md10'] = RandomForestRegressor(n_estimators=25,max_depth=10, min_samples_split=200,n_jobs=-1)\n",
    "y_models['rf_md3'] = RandomForestRegressor(n_estimators=25,max_depth=3, min_samples_split=200,n_jobs=-1)\n",
    "y_models['nn'] = MLPRegressor(alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1, max_iter=model_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_splits = 4\n",
    "aux_dictionary = {'n_bins': 2, 'n_trees':2, 'max_depth':2, \n",
    "                  'upper':0.999, 'lower':0.001,\n",
    "                  'bootstrapreps':100,\n",
    "                  'subsample_ratio':0.5}\n",
    "bootstrap_number = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_data()\n",
    "\n",
    "feature_list = [x for x in df.columns if 'x' in x]\n",
    "\n",
    "ols = st.ate.ols_vanilla(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "pbin = st.ate.propbinning(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "plm = st.ate.dml.dml_plm(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "irm = st.ate.dml.dml_irm(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "ip = st.ate.ipw(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_data()\n",
    "df['splits'] = np.random.choice(n_data_splits, len(df), replace=True)\n",
    "df = df.sort_values(by='splits')    \n",
    "\n",
    "## Predict Treatment\n",
    "that = st.predict_treatment_indicator(df, 'splits', n_data_splits, feature_list,'treatment',t_models['LogitCV'])\n",
    "df['that'] = that\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1, figsize=(9,3), sharex=True, sharey=True)\n",
    "ax.hist(df.loc[df.treatment==1]['that'], density=False, facecolor='g', alpha=0.25)\n",
    "ax.hist(df.loc[df.treatment==0]['that'], density=False, facecolor='b', alpha=0.25)\n",
    "control_range_to_remove = np.percentile(df.loc[df.treatment==1]['that'], q= 50) , np.percentile(df.loc[df.treatment==1]['that'], q= 99)\n",
    "print(control_range_to_remove)\n",
    "\n",
    "df = df.loc[ (df.treatment==1) | ( (df.that.between(control_range_to_remove[0],control_range_to_remove[1])==False) & (df.treatment==0) )   ]\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1, figsize=(9,3), sharex=True, sharey=True)\n",
    "ax.hist(df.loc[df.treatment==1]['that'], density=False, facecolor='g', alpha=0.25)\n",
    "ax.hist(df.loc[df.treatment==0]['that'], density=False, facecolor='b', alpha=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section4'></a>\n",
    "\n",
    "## Bootstrapping\n",
    "* Bootstrap results using random datasets when all three assumptions are satisfied.\n",
    "* Bootstrap results when the unconfoundedness assumption is violated. Do this by removing one fot the features from training.\n",
    "* Bootstrap results when the overlap assumption is violated. Do this by removing control observations with propensities near the median treatment obervation propensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_x = []\n",
    "pbin_x= []\n",
    "plm_x = []\n",
    "irm_x = []\n",
    "ipw_x = []\n",
    "\n",
    "ols_x_unconf = []\n",
    "pbin_x_unconf= []\n",
    "plm_x_unconf = []\n",
    "irm_x_unconf = []\n",
    "ipw_x_unconf = []\n",
    "\n",
    "ols_x_overlap = []\n",
    "pbin_x_overlap= []\n",
    "plm_x_overlap = []\n",
    "irm_x_overlap = []\n",
    "ipw_x_overlap = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(bootstrap_number):\n",
    "    df = generate_data()\n",
    "    \n",
    "    feature_list = [x for x in df.columns if 'x' in x]\n",
    "    \n",
    "    feature_list_ab = [x for x in feature_list if '3' not in x and '4' not in x]\n",
    "    \n",
    "    ## Regular \n",
    "    ols = st.ate.ols_vanilla(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    pbin = st.ate.propbinning(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    plm = st.ate.dml.dml_plm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    irm = st.ate.dml.dml_irm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    ip = st.ate.ipw(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "    ols_x.append(ols['ATE TE'])\n",
    "    pbin_x.append(pbin['ATE TE'])\n",
    "    plm_x.append(plm['ATE TE'])\n",
    "    irm_x.append(irm['ATE TE'])    \n",
    "    ipw_x.append(ip['ATE TE'])   \n",
    "    \n",
    "    ## When unconfoundedness assumption is not true\n",
    "    ols = st.ate.ols_vanilla(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    pbin = st.ate.propbinning(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    plm = st.ate.dml.dml_plm(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    irm = st.ate.dml.dml_irm(df, \n",
    "                    'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    ip = st.ate.ipw(df, \n",
    "                'splits', feature_list_ab, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "    ols_x_unconf.append(ols['ATE TE'])\n",
    "    pbin_x_unconf.append(pbin['ATE TE'])\n",
    "    plm_x_unconf.append(plm['ATE TE'])\n",
    "    irm_x_unconf.append(irm['ATE TE'])    \n",
    "    ipw_x_unconf.append(ip['ATE TE'])        \n",
    "\n",
    "\n",
    "    ## When overlap condition is not true\n",
    "    df['splits'] = np.random.choice(n_data_splits, len(df), replace=True)\n",
    "    df = df.sort_values(by='splits')    \n",
    "    ## Predict Treatment\n",
    "    that = st.predict_treatment_indicator(df, 'splits', n_data_splits, feature_list,'treatment',t_models['LogitCV'])\n",
    "    df['that'] = that    \n",
    "    control_range_to_remove = np.percentile(df.loc[df.treatment==1]['that'], q= 50) , np.percentile(df.loc[df.treatment==1]['that'], q= 99)\n",
    "    df = df.loc[ (df.treatment==1) | ( (df.that.between(control_range_to_remove[0],control_range_to_remove[1])==False) & (df.treatment==0) )   ]\n",
    "\n",
    "\n",
    "    ols = st.ate.ols_vanilla(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    pbin = st.ate.propbinning(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    plm = st.ate.dml.dml_plm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    irm = st.ate.dml.dml_irm(df, \n",
    "                    'splits', feature_list, 'Y', 'treatment',\n",
    "                    y_models['LassoCV'],t_models['LogitCV'],\n",
    "                   n_data_splits, aux_dictionary )\n",
    "    ip = st.ate.ipw(df, \n",
    "                'splits', feature_list, 'Y', 'treatment',\n",
    "                y_models['LassoCV'],t_models['LogitCV'],\n",
    "               n_data_splits, aux_dictionary )\n",
    "\n",
    "    ols_x_overlap.append(ols['ATE TE'])\n",
    "    pbin_x_overlap.append(pbin['ATE TE'])\n",
    "    plm_x_overlap.append(plm['ATE TE'])\n",
    "    irm_x_overlap.append(irm['ATE TE'])    \n",
    "    ipw_x_overlap.append(ip['ATE TE'])        \n",
    "\n",
    "ols_x = np.array(ols_x) - 5\n",
    "pbin_x = np.array(pbin_x) - 5\n",
    "plm_x = np.array(plm_x) - 5\n",
    "irm_x = np.array(irm_x) - 5\n",
    "ipw_x = np.array(ipw_x) - 5\n",
    "\n",
    "ols_x_unconf = np.array(ols_x_unconf) - 5\n",
    "pbin_x_unconf = np.array(pbin_x_unconf) - 5\n",
    "plm_x_unconf = np.array(plm_x_unconf) - 5\n",
    "irm_x_unconf = np.array(irm_x_unconf) - 5\n",
    "ipw_x_unconf = np.array(ipw_x_unconf) - 5    \n",
    "\n",
    "ols_x_overlap = np.array(ols_x_overlap) - 5\n",
    "pbin_x_overlap = np.array(pbin_x_overlap) - 5\n",
    "plm_x_overlap = np.array(plm_x_overlap) - 5\n",
    "irm_x_overlap = np.array(irm_x_overlap) - 5\n",
    "ipw_x_overlap = np.array(ipw_x_overlap) - 5    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_avg_med_iqr(x):\n",
    "    avg = np.average(x)\n",
    "    p50 = np.percentile(x, 50)\n",
    "    p25 = np.percentile(x, 25)\n",
    "    p75 = np.percentile(x, 75)    \n",
    "    print('AVG: {0:5.2f}   MED: {1:5.2f}   IQR: [{2:5.3f}, {3:5.2f}]'.format(avg, p50, p25, p75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bias when all assumptions are met')\n",
    "print_avg_med_iqr(ols_x)    \n",
    "print_avg_med_iqr(pbin_x)    \n",
    "print_avg_med_iqr(plm_x)    \n",
    "print_avg_med_iqr(irm_x)    \n",
    "print_avg_med_iqr(ipw_x)    \n",
    "\n",
    "print('')\n",
    "print('Bias when unconfoundedness is not met')\n",
    "print_avg_med_iqr(ols_x_unconf) \n",
    "print_avg_med_iqr(pbin_x_unconf)    \n",
    "print_avg_med_iqr(plm_x_unconf)    \n",
    "print_avg_med_iqr(irm_x_unconf)    \n",
    "print_avg_med_iqr(ipw_x_unconf)    \n",
    "\n",
    "print('')\n",
    "print('Bias when overlap is not met')\n",
    "print_avg_med_iqr(ols_x_overlap)    \n",
    "print_avg_med_iqr(pbin_x_overlap)    \n",
    "print_avg_med_iqr(plm_x_overlap)    \n",
    "print_avg_med_iqr(irm_x_overlap)    \n",
    "print_avg_med_iqr(ipw_x_overlap[~np.isnan(ipw_x_overlap)])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section5'></a>\n",
    "\n",
    "## Prediction vs Causal\n",
    "Let's compare the estimated treatment effects $\\hat{Y}(W=1) - \\hat{Y}(W=0) $ among ML models. Let's use the treatment effect of multiple features.\n",
    "\n",
    "We expect to find evidence of regularization bias. As demonstrated in **Figure 1** of Chernozhukov et al., \"*Double/Debiased Machine Learning for Treatment and Structural Parameters*\" (https://arxiv.org/pdf/1608.00060.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data(WDim=2,\n",
    "             TE = [1,1],\n",
    "             N = 50):\n",
    "    corr = False\n",
    "    if corr==False:\n",
    "        pass\n",
    "    else:\n",
    "        x = np.random.uniform(0,1,N)\n",
    "        \n",
    "    for r in range(WDim+1):\n",
    "        if corr==False:\n",
    "            W = np.random.randint(0,2, N)     \n",
    "        else: \n",
    "            x1 = np.random.uniform(-1,1,N)                \n",
    "            W = ( ( np.exp(x + x1) / (1+ np.exp(x+x1)) ) > np.random.uniform(0.45,0.55) ).astype(float)\n",
    "        if r ==0:\n",
    "            Y = TE[r]*W + np.random.normal(0,1, N)\n",
    "            data_dict = {'W1':W}\n",
    "        else:\n",
    "            Y = TE[r]*W\n",
    "            data_dict['W'+str(r)] = W\n",
    "    data_dict['Y'] = Y\n",
    "    return pd.DataFrame(data=data_dict, index=np.arange(N))\n",
    "#     if corr==False:\n",
    "#         W1 = np.random.randint(0,2, N) \n",
    "#         W2 = np.random.randint(0,2, N)     \n",
    "#     else:\n",
    "#         x = np.random.uniform(0,1,N)\n",
    "#         x1 = np.random.uniform(-1,1,N)\n",
    "#         x2 = np.random.uniform(-1,1,N)        \n",
    "#         W1 = ( ( np.exp(x + x1) / (1+ np.exp(x+x1)) ) > np.random.uniform(0.45,0.55) ).astype(float)\n",
    "#         W2 = ( ( np.exp(x + x2) / (1+ np.exp(x+x2)) ) > np.random.uniform(0.45,0.55) ).astype(float)        \n",
    "\n",
    "\n",
    "#     Y = TE[0]*W1 + TE[1]*W2 + np.random.normal(0,1, N)\n",
    "#     return pd.DataFrame(data={'Y':Y, 'W1':W1, 'W2': W2}, index=np.arange(N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_te(X, WDim, func):\n",
    "    trained = func.fit(X[[x for x in X.columns if 'W' in x]], X['Y'])\n",
    "    te_output = {}\n",
    "    for r in range(WDim+1):\n",
    "        T = np.zeros(WDim)\n",
    "        T[r-1] = 1\n",
    "        te_output[str(r)] = trained.predict([T])[0] - trained.predict([np.zeros(WDim)])[0]\n",
    "#     te1 = trained.predict([[1,0]]) - trained.predict([[0,0]])\n",
    "#     te2 = trained.predict([[0,1]]) - trained.predict([[0,0]])    \n",
    "    return te_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2 = MLPRegressor(hidden_layer_sizes=(2,), max_iter = 2000, random_state=4227)\n",
    "nn10 = MLPRegressor(hidden_layer_sizes=(10,), max_iter = 2000, random_state=4227)\n",
    "ols = LinearRegression()\n",
    "rf1000 = RandomForestRegressor(n_estimators=1000)\n",
    "rf100 = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WDim_use = 5\n",
    "TE_use = [0.50]*(WDim_use+1)\n",
    "sim_range = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Data and simulate many OLS and other estimates\n",
    "dict_est = {}\n",
    "for w in range(WDim_use):\n",
    "    dict_est[str(w)] = {'OLS':[], 'NN2':[], 'NN10':[], 'RF1000':[], 'RF100':[]  }\n",
    "\n",
    "for r in range(sim_range):\n",
    "    df = sim_data(WDim=WDim_use, TE = TE_use, N = 1000)\n",
    "    # display(df.describe())\n",
    "    ols_HAT = ml_te(df,   WDim_use, ols)\n",
    "    nn2_HAT = ml_te(df,   WDim_use, nn2)\n",
    "    nn10_HAT = ml_te(df,  WDim_use, nn10)\n",
    "    rf1000_HAT = ml_te(df,WDim_use, rf1000)\n",
    "    rf100_HAT = ml_te(df, WDim_use, rf100)\n",
    "    for w in range(WDim_use):\n",
    "        dict_est[str(w)]['OLS'].append(ols_HAT[str(w)])\n",
    "        dict_est[str(w)]['NN2'].append(nn2_HAT[str(w)])\n",
    "        dict_est[str(w)]['NN10'].append(nn10_HAT[str(w)])\n",
    "        dict_est[str(w)]['RF1000'].append(rf1000_HAT[str(w)])\n",
    "        dict_est[str(w)]['RF100'].append(rf100_HAT[str(w)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(WDim_use):\n",
    "    print('For Parameter {0}'.format(w))\n",
    "    for a in ['OLS','NN2','NN10','RF1000','RF100']:\n",
    "        print(a)\n",
    "        est1_bias = np.abs( np.mean(dict_est[str(w)][a] ) - TE_use[w] )\n",
    "        print(' |Bias| of Est = {0:5.3} using {1}'.format(est1_bias, a))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,ncols=2, figsize=(15,8), sharex=True, sharey=True)\n",
    "ax[0,0].hist(dict_est['1']['OLS'],   bins=20, density=False,  label='ols',           alpha=0.25, color='coral')\n",
    "ax[0,0].hist(dict_est['1']['NN2'],   bins=20, density=False,  label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "# ax[0,0].hist(dict_est['1']['NN10'],  bins=20, density=False,  label='neural net 10', alpha=0.25, color='lime')\n",
    "# ax[0,0].hist(dict_est['1']['RF1000'],bins=20, density=False,  label='RF 1000',       alpha=0.25,  color='navy')\n",
    "ax[0,0].hist(dict_est['1']['RF100'], bins=20, density=False,  label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "# ax[0,0].vlines(TE_use[0], 0, 10, colors='black', label='Truth')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].grid()\n",
    "ax[0,0].set_title('Estimate 1')\n",
    "\n",
    "ax[1,0].hist(dict_est['1']['OLS'],   bins=20, density=False,  label='ols',           alpha=0.25, color='coral')\n",
    "# ax[1,0].hist(dict_est['1']['NN2'],   bins=20, density=False,  label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "ax[1,0].hist(dict_est['1']['NN10'],  bins=20, density=False,  label='neural net 10', alpha=0.25, color='lime')\n",
    "ax[1,0].hist(dict_est['1']['RF1000'],bins=20, density=False,  label='RF 1000',       alpha=0.25,  color='navy')\n",
    "# ax[1,0].hist(dict_est['1']['RF100'], bins=20, density=False,  label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "# ax[1,0].vlines(TE_use[0], 0, 10, colors='black', label='Truth')\n",
    "ax[1,0].legend()\n",
    "ax[1,0].grid()\n",
    "ax[1,0].set_title('Estimate 1')\n",
    "\n",
    "ax[0,1].hist(dict_est['2']['OLS'],   bins=20, density=False, label='ols',           alpha=0.25, color='coral')\n",
    "ax[0,1].hist(dict_est['2']['NN2'],   bins=20, density=False, label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "# ax[0,1].hist(dict_est['2']['NN10'],  bins=20, density=False, label='neural net 10', alpha=0.25, color='lime')\n",
    "# ax[0,1].hist(dict_est['2']['RF1000'],bins=20, density=False, label='RF 1000',       alpha=0.25,  color='navy')\n",
    "ax[0,1].hist(dict_est['2']['RF100'], bins=20, density=False, label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "# ax[0,1].vlines(TE_use[1], 0, 10, colors='black', label='Truth')\n",
    "ax[0,1].legend()\n",
    "ax[0,1].grid()\n",
    "ax[0,1].set_title('Estimate 2')\n",
    "\n",
    "ax[1,1].hist(dict_est['2']['OLS'],   bins=20, density=False, label='ols',           alpha=0.25, color='coral')\n",
    "# ax[1,1].hist(dict_est['2']['NN2'],   bins=20, density=False, label='neural net 2',  alpha=0.25, color='darkgreen')\n",
    "ax[1,1].hist(dict_est['2']['NN10'],  bins=20, density=False, label='neural net 10', alpha=0.25, color='lime')\n",
    "ax[1,1].hist(dict_est['2']['RF1000'],bins=20, density=False, label='RF 1000',       alpha=0.25,  color='navy')\n",
    "# ax[1,1].hist(dict_est['2']['RF100'], bins=20, density=False, label='RF 100',        alpha=0.25,  color='skyblue')\n",
    "# ax[1,1].vlines(TE_use[1], 0, 10, colors='black', label='Truth')\n",
    "ax[1,1].legend()\n",
    "ax[1,1].grid()\n",
    "ax[1,1].set_title('Estimate 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
